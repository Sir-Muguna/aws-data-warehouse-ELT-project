# ğŸš€ AWS Data Warehouse ELT Project

Project Summary: Modern ELT Pipeline with Airflow, Redshift & dbt.

This project showcases a production-grade ELT pipeline built on AWS, orchestrated with Apache Airflow, powered by Amazon Redshift, and modeled using dbt. Here's a breakdown of the full data journey:

ğŸ› ï¸ Bronze Layer: Schema & Raw Ingestion.

We kick off by building the foundational structure in the Bronze layer. Redshift DDL scripts create schemas and tables for CRM and ERP systems, followed by a clean-up phase that truncates existing data to ensure a fresh load. Then, raw CSV files are securely ingested from Amazon S3 into Redshift using highly-optimized COPY commands.

ğŸ”„ Silver Layer: Data Transformation with dbt.

Once the raw data lands, we level up to the Silver layer where dbt (data build tool) runs curated transformation models. These models clean, validate, and enrich the dataâ€”turning chaos into clarity. Post-transformation, automated tests are executed to ensure integrity and trustworthiness.

ğŸ† Gold Layer: Business-Centric Models & Final QA.

In the final stage, the Gold layer focuses on delivering business-ready datasets. Aggregations, KPIs, and domain-specific insights are built using dbt and tested rigorously to guarantee quality. This ensures analytics and downstream systems consume clean, tested, and high-value data.

âœ… From raw files in S3 to trusted business insights in Redshift, this pipeline follows the best practices of modern data engineeringâ€”automated, modular, and production-ready.

![Architecture](./include/docs/architecture.png)

---

## ğŸ§  Key Features

- âš™ï¸ **Airflow Orchestration**: Task scheduling, logging, and monitoring via DAGs.  
- ğŸª£ **Data Lake on S3**: Stores raw and intermediate files (Bronze & Silver layers).  
- ğŸ§½ **ELT Design**: Transformation logic is handled in Redshift using dbt.  
- ğŸ›¢ **Data Warehouse**: Redshift holds the final star-schema (Gold layer) optimized for analytics.  
- ğŸ“ˆ **Modular and Scalable**: Built for extensibility and cloud-scale workloads.

---

## ğŸ—‚ï¸ Project Architecture

This project follows the **Medallion Architecture** pattern using AWS-native services and transformation layers powered by **dbt**.

### ğŸ§± Medallion Architecture Layers

1. **Bronze (DDL Definitions)**: Create base staging tables in Redshift.  
2. **Silver (Cleaned Data)**: Load and clean raw data using SQL models in dbt.  
3. **Gold (Data Mart)**: Star schema (facts and dimensions) ready for analytics.

![Tables](./include/docs/redshift.png)

![StarSchema](./include/docs/data_model.png)
---

## ğŸ” DAG Workflow

The main DAG `load_s3_data_to_redshift.py` orchestrates the entire ELT flow:

1. **Stage to Redshift**: Copies raw CSV files from S3 to Redshift.  
2. **DDL Creation (Bronze)**: Executes Redshift SQL scripts to create schema and base tables.  
3. **Run dbt Models**:  
   - **Silver Layer**: Cleans and prepares the data.  
   - **Gold Layer**: Builds dimension and fact tables.  
4. **Data Quality Checks** *(using dbt test)*: Ensure integrity before marking pipeline as successful.

![Airflow](./include/docs/airflow.png)
---

## ğŸ“ Repository Structure

```bash
aws-data-warehouse-ELT-project/
â”‚
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ load_s3_data_to_redshift.py       # Airflow DAG
â”‚
â”œâ”€â”€ include/
â”‚   â””â”€â”€ datasets/                         # Optional: raw datasets (local testing)
â”‚
â”œâ”€â”€ dbt/
â”‚   â”œâ”€â”€ dbt_packages/
â”‚   â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ macros/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ gold/                         # Final dimensional models
â”‚   â”‚   â”‚   â”œâ”€â”€ dim_customers.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ dim_products.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ fact_sales.sql
â”‚   â”‚   â”‚   â””â”€â”€ schema.yml
â”‚   â”‚   â””â”€â”€ silver/                       # Cleaned data layer
â”‚   â”‚       â”œâ”€â”€ crm_cust_info.sql
â”‚   â”‚       â”œâ”€â”€ crm_prd_info.sql
â”‚   â”‚       â”œâ”€â”€ crm_sales_details.sql
â”‚   â”‚       â”œâ”€â”€ erp_cust_atz12.sql
â”‚   â”‚       â”œâ”€â”€ erp_loc_a101.sql
â”‚   â”‚       â”œâ”€â”€ erp_px_cat_g1v2.sql
â”‚   â”‚       â”œâ”€â”€ schema.yml
â”‚   â”‚       â””â”€â”€ sources.yml
â”‚   â”œâ”€â”€ target/                           # dbt compiled/run outputs
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â”œâ”€â”€ profiles.yml
â”‚   â”œâ”€â”€ packages.yml
â”‚   â””â”€â”€ package-lock.yml
â”‚
â”œâ”€â”€ logs/                                 # Airflow log directory
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ ddl_bronze/                       # Redshift DDL scripts
â”‚       â”œâ”€â”€ create_crm_cust_info.sql
â”‚       â”œâ”€â”€ create_crm_prd_info.sql
â”‚       â”œâ”€â”€ create_crm_sales_details.sql
â”‚       â”œâ”€â”€ create_erp_cust_atz12.sql
â”‚       â”œâ”€â”€ create_erp_loc_a101.sql
â”‚       â”œâ”€â”€ create_erp_px_cat_g1v2.sql
â”‚       â”œâ”€â”€ create_schema.sql
â”‚       â”œâ”€â”€ grant_allusersql.sql
â”‚       â””â”€â”€ grant_usage_public.sql

